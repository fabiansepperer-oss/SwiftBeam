videomae_train system_info

Directory overview
- benchmark.sh: Runs short throughput benchmarks using src/train.py with --bench_only, includes CUDA probing and writes JSON benchmark results under outputs/<exp_name>/logs.
- configs/: YAML configuration files for base runs and stage overrides. base_T8_192.yaml and base_T16_192.yaml set dataset, model, and training defaults. paths.yaml centralizes data_root, outputs_root, and pretrained_dir. stageA_head_only.yaml and stageB_unfreeze_last2.yaml override base settings for staged fine-tuning.
- data/: Expected dataset root referenced by configs/paths.yaml. The layout is data/dataset/{train,val,test}/<class_name>/*.mp4.
- outputs/: Training outputs. train.py creates outputs/<exp_name> with subfolders checkpoints/, logs/, and results/.
- pretrained/: Location for any pretrained assets referenced by configuration.
- run_train.sh: Convenience script that runs train.py for base config and two staged overrides.
- scripts/: Auxiliary scripts (not referenced by train.py in this repo snapshot).
- src/: Python source for data loading, model creation, metrics, evaluation, training, and utilities.
- README.md and requirements.txt: Documentation (currently empty) and dependency list.

Training workflow and file interactions
1) Entry points: run_train.sh or direct calls to src/train.py. run_train.sh calls train.py three times with different config/override combinations. benchmark.sh calls train.py in benchmark-only mode to measure throughput.
2) Configuration: train.py uses utils.load_config to read a base YAML, optionally merge override YAML, and resolve a shared paths.yaml include. The merged config dictates data locations, model choice, hyperparameters, and output paths.
3) Dataset: train.py builds DataLoaders via make_loader, which instantiates dataset.VideoFolderDataset. VideoFolderDataset uses list_videos to scan data_root/{train,val,test} for class folders and .mp4 files, then uses load_clip_opencv to read and resize frames, normalize them, and return torch tensors.
4) Model: train.py calls model.build_model. If transformers VideoMAE is available, it loads a pretrained VideoMAE model; otherwise it falls back to a torchvision R3D-18 backbone. maybe_freeze can freeze the backbone for staged training.
5) Training: train.py runs the training loop with AMP and an AdamW optimizer. It logs per-step loss to outputs/<exp_name>/logs/train.log via utils.append_jsonl and periodically evaluates with eval.run_eval.
6) Evaluation: eval.run_eval computes loss and classification metrics using metrics.compute_classification_metrics, then writes per-epoch metrics to outputs/<exp_name>/results/val_metrics.jsonl.
7) Checkpointing: utils.save_checkpoint writes outputs/<exp_name>/checkpoints/best.pt and last.pt. A config snapshot and summary JSON files are also written to outputs/<exp_name>/checkpoints/config_snapshot.json and outputs/<exp_name>/results/best_val_summary.json. Optional test metrics are written to outputs/<exp_name>/results/test_metrics.json.

Function reference (src/*.py)

utils.py

Function: seed_everything(seed)
This helper centralizes all randomness seeding used by the training pipeline. It sets the Python standard library RNG, NumPy RNG, and PyTorch RNG to the same integer seed so that common sources of nondeterminism are aligned. It also sets the PYTHONHASHSEED environment variable to reduce nondeterministic hashing behaviors in Python, which can influence iteration order for sets and dicts in some contexts. For CUDA, it invokes torch.cuda.manual_seed_all so all visible GPUs get the same seed value. This function is called at the top of train.py after configuration is loaded, so the rest of the run starts from a controlled random state. It does not toggle cuDNN determinism flags or benchmarking, so full determinism still depends on PyTorch backend settings and the specific operators used. Its role is to reduce run-to-run variation, aid reproducibility, and make comparisons between configuration changes more meaningful.

Function: ensure_dir(p)
This is a small filesystem utility that guarantees a directory exists before other code tries to write to it. It accepts either a string path or a pathlib.Path, wraps the value in Path, and calls mkdir with parents=True and exist_ok=True. That combination means it will create any missing parent directories and will not throw an error if the directory already exists. train.py uses ensure_dir to create outputs/<exp_name>/checkpoints, logs, and results at startup, which prevents later file-writing calls from failing. In this project it is used only for folders, not files, so it does not check if the path already points to a file. It also avoids side effects like removing existing content, so it is safe to call repeatedly across runs. The function keeps higher-level code clean and lets callers focus on the logic of logging and checkpointing rather than boilerplate directory checks.

Function: now_ts()
This helper returns a human-readable timestamp string formatted as YYYY-MM-DD_HH-MM-SS. It uses time.strftime with the current local time. The function is not used in the current train.py flow, but it is a common utility for naming runs, files, or output directories in a reproducible format. The underscore between date and time avoids whitespace that can be awkward in filenames. If added to the training workflow, it could be used to create unique experiment names or to timestamp log files without worrying about platform-specific formatting. It returns a plain string with no timezone information, so if cross-timezone comparison matters, the calling code would need to handle that separately. The function is intentionally tiny and predictable, so it is easy to use in a logging context without introducing side effects.

Function: load_yaml(path)
This function loads a YAML file from disk and returns it as a Python dictionary. It imports yaml locally to keep the top-level import list minimal, opens the file at the provided path in text mode, and calls yaml.safe_load to parse the contents. The use of safe_load prevents execution of arbitrary code in the YAML and limits the parsing to standard YAML types. The returned structure is a dict with nested values based on the YAML file content. train.py uses load_yaml indirectly through load_config to read configuration files in configs/. The function is simple but central: if the YAML fails to parse or the file path is wrong, training will fail early and clearly. It does not provide any schema validation, so callers must ensure expected keys exist. This separation also makes it easier to unit test configuration logic in isolation if needed.

Function: merge_dicts(base, override)
This function merges two dictionaries in a shallow, override-first manner. It starts by copying the base dict, then iterates over the override dict and assigns each key and value into the output. If a key exists in both dicts, the override value replaces the base value. It does not recursively merge nested dicts; nested dicts are replaced wholesale. This is enough for the current configuration style where overrides tend to be top-level keys. It is used in load_config to combine a paths include with a main config, and again to apply an override config on top of the base. The function intentionally avoids mutating the input dictionaries, which makes it safer for reuse and avoids surprising side effects if the original dicts are referenced elsewhere. Its simplicity keeps configuration logic understandable and predictable.

Function: load_config(main_cfg_path, override_cfg_path=None)
This is the configuration orchestrator for the training pipeline. It loads the main config YAML, then checks for a special case: if the config contains a key named "paths" and its value is a string, it treats that as a path to another YAML file. It loads that paths YAML, merges it into the main config, and removes the paths key. This pattern centralizes path values while still allowing each base config to reference them. If an override config is provided, it is loaded and merged last, so its values take precedence. The function returns a single dictionary containing all settings the rest of the code expects. It is used in train.py before any training logic runs, which means it controls dataset locations, hyperparameters, model choice, and output directories. The merging is shallow, so if you add nested structures in configs you should account for full replacement behavior.

Function: AverageMeter.update(v, n=1)
AverageMeter is a small dataclass for accumulating values and counts, and update is the method that records new observations. It multiplies the incoming value by n and adds it to the running sum stored in self.value, then increments self.count by n. This design lets callers update the meter with either a single sample or a batch of samples where v is an average and n is the batch size. The method does not compute the average directly; it only updates the internal totals. AverageMeter is not currently used in train.py, but it could be used to track losses, accuracies, or timing statistics in a more structured way than manual accumulators. The method does not validate input types or values, so callers should pass numeric values. It is intentionally minimal and fast, appropriate for tight training loops where overhead should be low.

Function: AverageMeter.avg
This property computes the current average from the accumulated value and count. If count is zero, it returns 0.0 to avoid division by zero. Otherwise it returns value divided by count, providing a mean value that reflects all updates so far. Because it is a property, callers use it like meter.avg instead of meter.avg(), which makes code more readable. The method does not reset the meter; resetting would require directly setting value and count or creating a new AverageMeter instance. In the context of training, it could be queried at the end of an epoch or at logging intervals to provide smooth metrics. It uses float arithmetic and does not format the output, leaving presentation to the caller. The design keeps the class simple and avoids coupling to any logging or reporting framework.

Function: save_json(path, obj)
This function serializes a Python object to JSON and writes it to disk. It opens the target path in write mode and calls json.dump with indent=2 for readable output. It expects obj to be JSON-serializable; for example, dicts, lists, numbers, strings, and nested combinations of these. It is used in train.py to write config snapshots, benchmark statistics, and summary metrics like best_val_summary.json and test_metrics.json. The function overwrites any existing file at the given path; it does not append. It also does not create parent directories, so callers must ensure the directory exists, typically using ensure_dir. The consistent formatting makes files easy to diff and inspect. The function keeps JSON writing behavior consistent across the codebase and reduces repeated boilerplate.

Function: append_jsonl(path, obj)
This function appends one JSON-encoded record to a file in JSON Lines format. It opens the file in append mode and writes json.dumps(obj) followed by a newline. JSON Lines is convenient for streaming logs because each line is a standalone JSON object and can be processed incrementally. train.py uses append_jsonl to record training loss and learning rate into outputs/<exp_name>/logs/train.log and to append validation metrics into outputs/<exp_name>/results/val_metrics.jsonl. Because it appends, the file grows over time and can be tailed or parsed without loading the entire log into memory. The function does not validate the object; it assumes it is serializable. It also does not add file headers, so the logs remain a pure line-delimited stream suitable for scripts and plotting tools.

Function: save_checkpoint(ckpt_path, model, optimizer, scaler, epoch, best_val, cfg)
This function packages the full training state into a single checkpoint file using torch.save. It builds a payload dict with the current epoch, best validation score, the model state_dict, optimizer state_dict, and the configuration dict. If a GradScaler is provided, it also stores the scaler state to preserve AMP scaling for resuming. train.py calls this for both best.pt and last.pt so you can resume training or deploy the best model. The function relies on PyTorch serialization, which writes a binary file. It does not include RNG states or scheduler state, so a resumed run may not be bit-for-bit identical, but it preserves the main training artifacts. It expects ckpt_path to be a valid path and does not create directories, so ensure_dir should be used earlier. The function centralizes checkpoint saving to keep train.py clean and consistent.


dataset.py

Function: list_videos(root)
This function scans the dataset directory for class-labeled videos and returns a list of samples plus class names. It treats each immediate subdirectory of root as a class label, sorts those names for stable ordering, and builds a mapping from class name to integer index. It then walks each class directory recursively with rglob("*.mp4"), creating a Sample dataclass instance for every video and attaching the class index as the label. If no videos are found, it raises a RuntimeError to fail fast, which prevents training on an empty dataset by mistake. The function returns both the sample list and the class name list, which can be useful for reporting or mapping predictions back to labels. It assumes all .mp4 files under class directories belong to that class and ignores other file types. This function underpins dataset construction and determines the label ordering used during training and evaluation.

Function: _read_frame_at(cap, frame_idx)
This internal helper extracts a single frame from an OpenCV VideoCapture. It seeks to the requested frame index using cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx), reads a frame, and returns None if the read fails. When a frame is read successfully, it converts the color space from BGR to RGB using cv2.cvtColor, matching the expected channel order for most deep learning models and the normalization statistics used later. Because VideoCapture is stateful, this helper encapsulates the seek and read steps and ensures consistent color conversion. It is used exclusively by load_clip_opencv when sampling a set of frames from a video. The function does not resize, normalize, or handle padding; it only reads one frame. It also does not release the capture object, leaving lifecycle management to the caller. This focused design keeps clip loading logic modular and easier to maintain.

Function: load_clip_opencv(video_path, num_frames, size)
This is the core video loading function that constructs a fixed-length clip tensor from a video file. It opens the video with cv2.VideoCapture, verifies it is readable, and obtains the total frame count. It then chooses frame indices uniformly across the video if the video is long enough; otherwise it pads by repeating the last available frame index until it reaches num_frames. It reads each selected frame via _read_frame_at, and if a frame fails to load it falls back to the previous frame or a zero image if none exist yet. Each frame is resized to size x size using bilinear interpolation to create consistent input dimensions. The frames are stacked into a NumPy array with shape [T, H, W, C] and dtype uint8. The function returns this array without normalization; normalization happens in VideoFolderDataset.__getitem__. It ensures that even short or partially corrupted videos produce a full clip, which helps keep training robust.

Function: VideoFolderDataset.__init__(split_root, num_frames, input_size)
The dataset constructor prepares the dataset for a particular split (train, val, or test). It calls list_videos to build a list of Sample objects and a class name list, which it stores as instance attributes. It also stores num_frames and input_size, which control how many frames are loaded per sample and the spatial resolution of each frame. The constructor sets mean and std arrays for normalization using the standard ImageNet statistics, which match the expectations of VideoMAE and many other vision models. It does not perform any expensive loading at init time beyond scanning the filesystem, so the dataset object can be created quickly. The dataset assumes videos are organized in class folders under split_root, and it expects .mp4 files. By keeping configuration on the instance, it allows train.py to build multiple datasets with different settings if needed. This method establishes the critical metadata used by __getitem__ when producing training samples.

Function: VideoFolderDataset.__len__()
This method returns the number of samples in the dataset. It simply returns the length of self.samples, which was populated in __init__ by list_videos. DataLoader uses this to determine epoch length and to size progress bars. The implementation is intentionally straightforward to keep performance overhead minimal. It assumes self.samples is a list of Sample dataclass instances and that the list remains unchanged for the lifetime of the dataset. Because the dataset is created at the start of training, changes to the filesystem after initialization will not be reflected unless you recreate the dataset. The method does not perform any filesystem checks or validation; all such work happens during dataset construction. It is a required method for PyTorch Dataset compatibility, and it allows downstream code to compute steps per epoch and schedule learning rate or evaluation intervals.

Function: VideoFolderDataset.__getitem__(idx)
This method loads and returns a single training example. It selects a Sample from self.samples by index, then calls load_clip_opencv to read a clip of num_frames frames at the configured input_size. The raw uint8 clip is converted to float32 and scaled to [0, 1]. It then applies per-channel normalization using the mean and std stored on the dataset, matching ImageNet normalization. The clip is converted to a torch tensor and permuted from [T, H, W, C] to [T, C, H, W], which is the layout expected by the model and training code. The label is converted to a torch.long tensor. It returns a tuple (clip, label) suitable for PyTorch DataLoader batching. The method does not apply random augmentations; all samples are deterministic given the video file. This design keeps preprocessing simple and consistent across train and eval splits, while still supporting variable-length videos through clip padding.


model.py

Function: R3D18Fallback.__init__(num_classes)
This constructor builds a lightweight fallback model for cases where the Hugging Face VideoMAE model is unavailable. It imports torchvision.models.video.r3d_18 with weights=None to get a randomly initialized 3D ResNet-18 backbone. It then reads the number of input features for the final fully connected layer and replaces that layer with a new Linear layer sized to the requested number of classes. The resulting model produces class logits for the dataset. This fallback is useful on systems without transformers installed or without network access to download pretrained weights. It provides a working baseline model, albeit likely less accurate than VideoMAE on many datasets. The constructor performs the minimal necessary customization and keeps the rest of the backbone intact. Because it uses torchvision defaults, it expects input tensors in the [B, C, T, H, W] layout, which is handled in the forward method. The class is only constructed via build_model when model_name is "r3d18_fallback" or when VideoMAE import fails.

Function: R3D18Fallback.forward(x)
The forward method adapts the input tensor layout and then delegates to the underlying 3D ResNet. In this project, clips are produced as [B, T, C, H, W], so the method permutes the tensor to [B, C, T, H, W] to match the expected layout of torchvision's r3d_18. It calls contiguous to ensure the tensor is stored in contiguous memory after the permutation, which can avoid issues and improve performance in downstream operations. It then passes the transformed tensor into the backbone and returns the resulting logits. There is no additional processing or activation; the caller is responsible for applying loss functions or softmax. This method keeps the fallback behavior as close as possible to a standard video classification model. By handling the layout conversion internally, it allows the rest of the training code to remain consistent regardless of whether VideoMAE or the fallback model is used.

Function: build_model(cfg)
This function builds the classification model specified by the configuration and returns both the model and a small preprocessing metadata dict. It reads model_name and num_classes from cfg and sets a preprocess_info dict indicating the expected input layout. If model_name is "videomae", it attempts to import transformers.VideoMAEForVideoClassification. If the import fails, it prints a warning and returns an R3D18Fallback instead. When VideoMAE is available, it optionally loads a VideoMAEConfig from the pretrained model to override image_size and num_frames based on cfg, then creates a model with ignore_mismatched_sizes=True to allow shape changes. It also optionally overrides dropout values in the model config. If model_name is explicitly "r3d18_fallback", it builds that model directly. Unknown model names raise a ValueError. This function encapsulates all model selection logic and keeps train.py agnostic to specific architectures. It is the main point for extending the system to new backbones.


metrics.py

Function: compute_classification_metrics(y_true, y_pred)
This function computes standard classification metrics given lists or arrays of true and predicted labels. It converts inputs to NumPy arrays for compatibility with scikit-learn, then computes accuracy with accuracy_score, F1 score with f1_score using average="binary", and a confusion matrix with confusion_matrix. It returns a MetricsResult dataclass containing the accuracy, F1 score, and confusion matrix. The use of average="binary" assumes a binary classification task; for multi-class problems this would need adjustment. The function is called from eval.run_eval after collecting predictions across the evaluation dataset. It keeps metric computation centralized and consistent. It does not handle probability thresholds; it assumes y_pred are class indices. The confusion matrix is returned as a NumPy array for later conversion to JSON via metrics_to_dict. Because it relies on scikit-learn, this function expects sklearn to be installed as specified in requirements.txt.

Function: metrics_to_dict(m)
This utility converts a MetricsResult dataclass into a JSON-serializable dictionary. It maps the acc and f1 floats directly and converts the confusion matrix array into a nested Python list using tolist(). This makes the output suitable for writing to JSON or JSON Lines logs via save_json or append_jsonl. It is used in eval.run_eval to merge metrics into the return dict alongside the loss. The function keeps serialization details out of the evaluation loop and ensures a consistent schema for logged metrics. It does not round values, so the full precision of the metrics is preserved in the output. The key names are stable (acc, f1, confusion_matrix), which helps downstream plotting or reporting tools parse results. The function is small but important for clean logging and reproducibility.


eval.py

Function: run_eval(model, loader, device)
This function evaluates a model on a dataset split without computing gradients. It is decorated with torch.no_grad to disable autograd, reducing memory use and speed. It switches the model to eval mode to disable dropout and other training-specific behavior. It iterates over the DataLoader, moves clips and labels to the specified device, runs the model, and extracts logits (supporting both Hugging Face models with a logits attribute and plain tensors). It computes cross-entropy loss and accumulates a weighted sum to later compute average loss. It also collects predicted class indices via argmax and records the true labels. After the loop, it calls compute_classification_metrics to get accuracy, F1, and confusion matrix, then returns a dict with loss and metrics serialized by metrics_to_dict. The function uses tqdm for a progress bar. Its output is consumed by train.py to log validation metrics and to select the best checkpoint based on F1 score.


train.py

Function: make_loader(split_root, cfg, shuffle)
This function builds a PyTorch DataLoader for a given dataset split. It constructs a VideoFolderDataset with the split_root path, num_frames, and input_size from the config. It sets the batch size based on whether the loader is for training (train_batch_size) or evaluation (val_batch_size). It then returns a DataLoader with shuffle enabled for training and disabled for evaluation, and sets num_workers from cfg with a default of 4. It uses pin_memory=True to speed up host-to-device transfers and drop_last=True for training to keep batch sizes consistent for optimizer steps. The function centralizes DataLoader settings so they can be adjusted in one place. It is called from main for train, val, and optionally test loaders. Because it uses the same dataset class for all splits, preprocessing and normalization stay consistent. Any data augmentation or sampling strategies would be added here or in the dataset class.

Function: maybe_freeze(model, cfg)
This function conditionally freezes the model backbone for staged fine-tuning. It reads freeze_backbone from the config and returns early if it is false. If the model has a videomae attribute, it sets requires_grad=False on all parameters of that submodule, which effectively freezes the backbone in Hugging Face VideoMAE. It then ensures the classifier parameters remain trainable by setting requires_grad=True on model.classifier if present. The function does not handle unfreezing specific layers based on unfreeze_last_n_blocks; that value exists in configs but is not used here. It is called after model construction and before creating the optimizer, so the optimizer only sees trainable parameters. The function is safe to call for fallback models as well; if they do not have videomae or classifier attributes, it simply does nothing beyond the initial check. This keeps the training logic flexible for different model types while supporting simple staged training setups.

Function: _sec_to_str(seconds)
This helper converts a duration in seconds into a more readable string. For durations under 60 seconds, it returns a value like "12.34s" with two decimal places. For durations under an hour, it formats the output as "Xm Ys" with minutes and one decimal place of seconds. For longer durations, it returns "Xh Ym Zs" with hours, minutes, and one decimal place of seconds. The function is used by print_benchmark_summary to provide friendly estimates of epoch and total training times based on benchmark step measurements. It keeps formatting logic out of the printing code and ensures consistent output formatting. The function does not round to whole seconds except for minutes and hours, so it still shows some precision for the seconds component. It is a small utility but improves the readability of benchmarking output for users running on different hardware.

Function: benchmark_steps(model, loader, optimizer, scaler, device, steps, amp, loss_fn)
This function runs a short, timed training loop to estimate throughput and performance breakdowns. It validates that steps is positive, optionally resets CUDA memory stats, and enters train mode. It iterates for the requested number of steps, with a warmup period to avoid skewing results with initialization overhead. For each step it measures data loading time, host-to-device transfer time, and compute time separately using time.perf_counter and CUDA synchronizations when on GPU. It performs a forward pass under autocast if amp is enabled, computes loss, scales gradients, performs an optimizer step, and updates the scaler. It records timing stats only after warmup, and computes averages, medians, steps per second, and clips per second based on actual batch sizes. It also captures peak CUDA memory usage when applicable. The function returns a metrics dict, which train.py uses to write a JSON file and optionally print a human-readable summary. It is used in benchmark-only mode and does not update any training logs or checkpoints.

Function: print_benchmark_summary(stats)
This function prints a formatted summary of benchmark statistics to stdout. It expects a dict with keys like bench_steps, avg_step_sec, median_step_sec, avg_load_sec, avg_h2d_sec, avg_compute_sec, steps_per_sec, and clips_per_sec. It prints a header, then a line describing step counts and average/median step times, then a breakdown of average load, transfer, and compute times. It also reports throughput in steps per second and clips per second along with average batch size. If CUDA memory statistics are present, it prints peak allocated and reserved memory in MiB; otherwise it indicates CPU. If estimated epoch and total durations are present, it formats and prints them using _sec_to_str. The function is used after benchmark_steps in bench-only runs to give immediate, readable feedback. It is purely a display helper and does not modify any state or files. The structure of its output makes it easy to scan in console logs or capture in shell scripts.

Function: main()
The main function orchestrates the full training workflow. It parses command-line arguments for config paths, optional overrides, benchmarking controls, and whether to run test evaluation after training. It loads configuration via load_config, seeds randomness, sets the device based on CUDA availability, creates output directories, and snapshots the merged config into checkpoints/config_snapshot.json. It computes train, val, and test dataset roots, constructs DataLoaders with make_loader, builds the model with build_model, optionally freezes the backbone, and moves the model to the target device. It creates an AdamW optimizer over trainable parameters, defines a cosine learning rate schedule with warmup via an inner lr_at function, and sets up AMP scaling and a cross-entropy loss. If bench_only is requested, it runs benchmark_steps, writes a JSON report, prints a summary, and exits. Otherwise, it executes the training loop across epochs, logging training loss to logs/train.log and validating with run_eval based on eval_every. It tracks the best validation F1, saves checkpoints, applies early stopping, writes a best_val_summary.json, and optionally evaluates on the test set. It is the central control flow for the entire training system.

Function: lr_at(step)
This nested function inside main computes the learning rate at a given global training step. It uses the base lr from config, applies a linear warmup for the first warmup_steps, and then applies cosine decay for the remainder of training. The warmup calculation scales linearly from base * (1/warmup_steps) up to base at the end of the warmup interval. After warmup, it computes t as the normalized progress through the remaining steps and uses the cosine schedule 0.5 * (1 + cos(pi * t)) to smoothly decay the learning rate to near zero by the final step. The function captures total_steps and warmup_steps from the surrounding scope, so it is tightly coupled to the configuration and dataset sizes computed in main. It is called each optimization step in the training loop to update the optimizer param group learning rates. Defining it inside main keeps the scheduler logic close to where its inputs are defined and reduces global namespace clutter.
